from langchain.agents import Tool, AgentExecutor, create_openai_functions_agent
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.memory import ConversationBufferMemory
from typing import Dict, Any, Optional, List
import httpx
import os
import numpy as np

# é…ç½®
KIMI_API_KEY = os.getenv("KIMI_API_KEY", "")
AUTODL_BASE_URL = os.getenv("AUTODL_BASE_URL", "http://localhost:7860")

os.environ["OPENAI_API_KEY"] = KIMI_API_KEY
os.environ["OPENAI_API_BASE"] = "https://api.moonshot.cn/v1"


class VoiceCloneOptimizer:
    """éŸ³è‰²å…‹éš†ä¼˜åŒ–å™¨ - æ”¯æŒå¤šéŸ³é¢‘èåˆå’Œå‚æ•°è°ƒèŠ‚"""
    
    def __init__(self):
        self.reference_audios = []  # å­˜å‚¨å¤šæ®µå‚è€ƒéŸ³é¢‘
        self.current_params = {
            "pitch": 0,           # éŸ³è°ƒ: -10 ~ +10
            "speed": 1.0,         # è¯­é€Ÿ: 0.5 ~ 2.0
            "timbre_depth": 0,    # éŸ³è‰²åšåº¦: -5 ~ +5
            "age_shift": 0,       # å¹´é¾„æ„Ÿ: -5(æ›´è€) ~ +5(æ›´å¹´è½»)
            "emotion_strength": 1.0  # æƒ…æ„Ÿå¼ºåº¦: 0.5 ~ 2.0
        }
        self.feedback_history = []
    
    def add_reference_audio(self, audio_bytes: bytes, description: str = ""):
        """æ·»åŠ å‚è€ƒéŸ³é¢‘"""
        self.reference_audios.append({
            "audio": audio_bytes,
            "description": description
        })
    
    def get_audio_count(self) -> int:
        """è·å–å·²ä¸Šä¼ éŸ³é¢‘æ•°é‡"""
        return len(self.reference_audios)
    
    def get_fused_embedding(self):
        """èåˆå¤šæ®µéŸ³é¢‘çš„ç‰¹å¾"""
        if len(self.reference_audios) == 0:
            return None
        if len(self.reference_audios) == 1:
            return self.reference_audios[0]["audio"]
        
        # TODO: å®ç°çœŸæ­£çš„ç‰¹å¾èåˆ
        # ç›®å‰ç®€å•è¿”å›ç¬¬ä¸€ä¸ªï¼Œå®é™…åº”è¯¥è°ƒç”¨æ¨¡å‹çš„èåˆAPI
        return self.reference_audios[0]["audio"]
    
    def adjust_params(self, feedback: str) -> Dict[str, Any]:
        """æ ¹æ®ç”¨æˆ·åé¦ˆè°ƒæ•´å‚æ•°"""
        feedback_lower = feedback.lower()
        adjustments = {}
        
        # å¹´é¾„ç›¸å…³
        if any(word in feedback_lower for word in ["å¹´è½»", "å«©", "å°å­©", "å¤ªå¹¼"]):
            self.current_params["age_shift"] -= 2
            adjustments["age_shift"] = self.current_params["age_shift"]
        elif any(word in feedback_lower for word in ["è€", "æˆç†Ÿ", "æ²§æ¡‘", "å¤ªè€"]):
            self.current_params["age_shift"] += 2
            adjustments["age_shift"] = self.current_params["age_shift"]
        
        # éŸ³è°ƒç›¸å…³
        if any(word in feedback_lower for word in ["å°–", "ç»†", "é«˜", "åˆºè€³"]):
            self.current_params["pitch"] -= 2
            self.current_params["timbre_depth"] += 1
            adjustments["pitch"] = self.current_params["pitch"]
            adjustments["timbre_depth"] = self.current_params["timbre_depth"]
        elif any(word in feedback_lower for word in ["ç²—", "åš", "ä½", "æ²‰", "é—·"]):
            self.current_params["pitch"] += 2
            self.current_params["timbre_depth"] -= 1
            adjustments["pitch"] = self.current_params["pitch"]
            adjustments["timbre_depth"] = self.current_params["timbre_depth"]
        
        # è¯­é€Ÿç›¸å…³
        if any(word in feedback_lower for word in ["å¿«", "æ€¥", "èµ¶"]):
            self.current_params["speed"] -= 0.2
            adjustments["speed"] = self.current_params["speed"]
        elif any(word in feedback_lower for word in ["æ…¢", "ç¼“", "æ‹–"]):
            self.current_params["speed"] += 0.2
            adjustments["speed"] = self.current_params["speed"]
        
        # æƒ…æ„Ÿå¼ºåº¦
        if any(word in feedback_lower for word in ["å¹³æ·¡", "æ²¡æ„Ÿæƒ…", "æœºæ¢°"]):
            self.current_params["emotion_strength"] += 0.3
            adjustments["emotion_strength"] = self.current_params["emotion_strength"]
        elif any(word in feedback_lower for word in ["å¤ªå¤¸å¼ ", "è¿‡ç«", "åšä½œ"]):
            self.current_params["emotion_strength"] -= 0.3
            adjustments["emotion_strength"] = self.current_params["emotion_strength"]
        
        self.feedback_history.append({
            "feedback": feedback,
            "adjustments": adjustments
        })
        
        return adjustments
    
    def get_optimization_suggestions(self) -> List[str]:
        """æ ¹æ®å½“å‰çŠ¶æ€ç»™å‡ºä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        if len(self.reference_audios) == 0:
            suggestions.append("ğŸ“¤ è¯·å…ˆä¸Šä¼ ä¸€æ®µå‚è€ƒéŸ³é¢‘ï¼ˆå»ºè®® 10-30 ç§’ï¼‰")
        elif len(self.reference_audios) == 1:
            suggestions.append("ğŸ’¡ æç¤ºï¼šä¸Šä¼  2-3 æ®µä¸åŒè¯­è°ƒ/æƒ…æ„Ÿçš„éŸ³é¢‘ï¼Œèåˆåæ•ˆæœæ›´ç¨³å®š")
        elif len(self.reference_audios) >= 3:
            suggestions.append(f"âœ… å·²ä¸Šä¼  {len(self.reference_audios)} æ®µéŸ³é¢‘ï¼Œèåˆæ•ˆæœè¾ƒå¥½")
        
        if len(self.feedback_history) > 0:
            suggestions.append(f"ğŸ“ å·²æ ¹æ® {len(self.feedback_history)} æ¬¡åé¦ˆä¼˜åŒ–å‚æ•°")
        
        return suggestions
    
    def reset_params(self):
        """é‡ç½®å‚æ•°"""
        self.current_params = {
            "pitch": 0,
            "speed": 1.0,
            "timbre_depth": 0,
            "age_shift": 0,
            "emotion_strength": 1.0
        }


class FishSpeechFunction:
    """Fish Speech - æ”¯æŒå‚æ•°è°ƒèŠ‚çš„ TTS"""
    
    name = "fish_speech_tts"
    description = """ä½¿ç”¨ Fish Speech åˆæˆè¯­éŸ³ï¼Œæ”¯æŒæƒ…æ„Ÿæ ‡ç­¾å’Œå‚æ•°è°ƒèŠ‚"""
    
    async def synthesize(
        self, 
        text: str, 
        reference_audio: Optional[bytes] = None,
        params: Optional[Dict] = None,
        temperature: float = 0.7
    ) -> bytes:
        """åˆæˆè¯­éŸ³ï¼Œæ”¯æŒå‚æ•°è°ƒèŠ‚"""
        
        # åº”ç”¨å‚æ•°è°ƒèŠ‚ï¼ˆé€šè¿‡æ–‡æœ¬æ ‡ç­¾æ¨¡æ‹Ÿï¼‰
        if params:
            # éŸ³è°ƒè°ƒèŠ‚
            pitch = params.get("pitch", 0)
            if pitch < -2:
                text = f"[pitch:low] {text}"
            elif pitch > 2:
                text = f"[pitch:high] {text}"
            
            # è¯­é€Ÿè°ƒèŠ‚
            speed = params.get("speed", 1.0)
            if speed < 0.9:
                text = f"[speed:slow] {text}"
            elif speed > 1.1:
                text = f"[speed:fast] {text}"
            
            # å¹´é¾„æ„Ÿï¼ˆé€šè¿‡æƒ…æ„Ÿæ ‡ç­¾æ¨¡æ‹Ÿï¼‰
            age_shift = params.get("age_shift", 0)
            if age_shift < -2:
                text = f"(soft) {text}"  # å¹´è½»æ„Ÿ
            elif age_shift > 2:
                text = f"(serious) {text}"  # æˆç†Ÿæ„Ÿ
        
        async with httpx.AsyncClient() as client:
            if reference_audio:
                files = {"reference_audio": ("audio.wav", reference_audio, "audio/wav")}
                data = {"text": text, "temperature": temperature}
                response = await client.post(
                    f"{AUTODL_BASE_URL}/tts", files=files, data=data, timeout=60.0
                )
            else:
                data = {"text": text, "temperature": temperature}
                response = await client.post(
                    f"{AUTODL_BASE_URL}/tts", json=data, timeout=60.0
                )
            
            if response.status_code == 200:
                return response.content
            raise Exception(f"Fish Speech å¤±è´¥: {response.text}")


class VoiceAgent:
    """è¯­éŸ³ Agent - æ”¯æŒäº¤äº’å¼ä¼˜åŒ–"""
    
    def __init__(self):
        self.llm = ChatOpenAI(model="moonshot-v1-8k", temperature=0.3)
        self.clone_optimizer = VoiceCloneOptimizer()
        self.tools = self._create_tools()
        
        self.agent = create_openai_functions_agent(
            llm=self.llm,
            tools=self.tools,
            prompt=self._create_prompt()
        )
        
        self.executor = AgentExecutor(
            agent=self.agent,
            tools=self.tools,
            memory=ConversationBufferMemory(memory_key="chat_history", return_messages=True),
            verbose=True
        )
    
    def _create_tools(self) -> list:
        """åˆ›å»ºå·¥å…·åˆ—è¡¨"""
        
        async def upload_reference_audio(audio_bytes: str, description: str = "") -> str:
            """ä¸Šä¼ å‚è€ƒéŸ³é¢‘ç”¨äºéŸ³è‰²å…‹éš†ã€‚æ”¯æŒå¤šæ¬¡ä¸Šä¼ ï¼Œå¤šæ®µéŸ³é¢‘ä¼šèåˆä½¿ç”¨ã€‚"""
            # æ³¨æ„ï¼šå®é™…åº”è¯¥æ¥æ”¶ bytesï¼Œè¿™é‡Œç®€åŒ–å¤„ç†
            self.clone_optimizer.add_reference_audio(audio_bytes.encode(), description)
            count = self.clone_optimizer.get_audio_count()
            
            if count == 1:
                return f"âœ… å·²ä¸Šä¼ ç¬¬ 1 æ®µéŸ³é¢‘ã€‚ğŸ’¡ å»ºè®®å†ä¸Šä¼  1-2 æ®µä¸åŒè¯­è°ƒçš„éŸ³é¢‘ï¼Œèåˆåæ•ˆæœæ›´ç¨³å®šã€‚"
            elif count == 2:
                return f"âœ… å·²ä¸Šä¼ ç¬¬ 2 æ®µéŸ³é¢‘ã€‚ğŸ’¡ å¯ä»¥å†ä¸Šä¼  1 æ®µï¼Œæˆ–å¼€å§‹åˆæˆã€‚"
            else:
                return f"âœ… å·²ä¸Šä¼ ç¬¬ {count} æ®µéŸ³é¢‘ï¼Œèåˆæ•ˆæœè¾ƒå¥½ï¼Œå¯ä»¥å¼€å§‹åˆæˆã€‚"
        
        async def synthesize_with_clone(text: str, emotion_tag: str = "") -> str:
            """ä½¿ç”¨å…‹éš†çš„éŸ³è‰²åˆæˆè¯­éŸ³ã€‚å¦‚æœå·²ä¸Šä¼ å¤šæ®µéŸ³é¢‘ï¼Œä¼šè‡ªåŠ¨èåˆã€‚"""
            if self.clone_optimizer.get_audio_count() == 0:
                return "âŒ è¯·å…ˆä¸Šä¼ å‚è€ƒéŸ³é¢‘"
            
            # è·å–èåˆåçš„éŸ³é¢‘
            fused_audio = self.clone_optimizer.get_fused_embedding()
            
            # åº”ç”¨å½“å‰å‚æ•°
            func = FishSpeechFunction()
            final_text = f"{emotion_tag} {text}" if emotion_tag else text
            
            audio = await func.synthesize(
                text=final_text,
                reference_audio=fused_audio,
                params=self.clone_optimizer.current_params
            )
            
            # ä¿å­˜å¹¶è¿”å›è·¯å¾„
            path = self._save_audio(audio, "cloned")
            suggestions = self.clone_optimizer.get_optimization_suggestions()
            
            return f"âœ… åˆæˆå®Œæˆï¼š{path}\n\n" + "\n".join(suggestions)
        
        async def optimize_voice(feedback: str) -> str:
            """æ ¹æ®åé¦ˆä¼˜åŒ–éŸ³è‰²ã€‚å¯ä»¥æè¿°é—®é¢˜å¦‚ï¼š'å¤ªå¹´è½»äº†'ã€'éŸ³è°ƒå¤ªå°–'ã€'è¯­é€Ÿå¤ªå¿«'ç­‰ã€‚"""
            adjustments = self.clone_optimizer.adjust_params(feedback)
            
            if not adjustments:
                return f"ğŸ¤” æœªèƒ½ç†è§£åé¦ˆï¼š'{feedback}'ã€‚è¯·å°è¯•æè¿°å…·ä½“ä¸€äº›ï¼Œå¦‚ï¼š\n- 'å¤ªå¹´è½»äº†'\n- 'éŸ³è°ƒå¤ªå°–'\n- 'è¯­é€Ÿå¤ªå¿«'\n- 'æƒ…æ„Ÿå¤ªå¹³æ·¡'"
            
            adjustment_desc = []
            for param, value in adjustments.items():
                if param == "pitch":
                    adjustment_desc.append(f"éŸ³è°ƒ {'é™ä½' if value < 0 else 'æé«˜'} åˆ° {value}")
                elif param == "speed":
                    adjustment_desc.append(f"è¯­é€Ÿ {'å‡æ…¢' if value < 1.0 else 'åŠ å¿«'} åˆ° {value:.1f}")
                elif param == "age_shift":
                    adjustment_desc.append(f"å¹´é¾„æ„Ÿ {'å¢åŠ ' if value > 0 else 'å‡å°‘'} åˆ° {value}")
                elif param == "timbre_depth":
                    adjustment_desc.append(f"éŸ³è‰²åšåº¦ {'å¢åŠ ' if value > 0 else 'å‡å°‘'} åˆ° {value}")
                elif param == "emotion_strength":
                    adjustment_desc.append(f"æƒ…æ„Ÿå¼ºåº¦è°ƒæ•´åˆ° {value:.1f}")
            
            return f"âœ… å·²æ ¹æ®åé¦ˆè°ƒæ•´ï¼š\n" + "\n".join(f"  - {desc}" for desc in adjustment_desc) + "\n\nè¯·é‡æ–°åˆæˆè¯­éŸ³æŸ¥çœ‹æ•ˆæœã€‚"
        
        async def get_optimization_tips() -> str:
            """è·å–ä¼˜åŒ–å»ºè®®"""
            suggestions = self.clone_optimizer.get_optimization_suggestions()
            current_params = self.clone_optimizer.current_params
            
            result = "ğŸ“Š å½“å‰çŠ¶æ€ï¼š\n"
            result += f"  - å‚è€ƒéŸ³é¢‘ï¼š{self.clone_optimizer.get_audio_count()} æ®µ\n"
            result += f"  - ä¼˜åŒ–æ¬¡æ•°ï¼š{len(self.clone_optimizer.feedback_history)} æ¬¡\n\n"
            result += "ğŸ”§ å½“å‰å‚æ•°ï¼š\n"
            for param, value in current_params.items():
                result += f"  - {param}: {value}\n"
            result += "\nğŸ’¡ å»ºè®®ï¼š\n"
            result += "\n".join(f"  - {s}" for s in suggestions)
            
            return result
        
        async def reset_voice_params() -> str:
            """é‡ç½®æ‰€æœ‰å‚æ•°åˆ°é»˜è®¤å€¼"""
            self.clone_optimizer.reset_params()
            return "âœ… å‚æ•°å·²é‡ç½®ä¸ºé»˜è®¤å€¼"
        
        return [
            Tool(
                name="upload_reference_audio",
                func=lambda x: upload_reference_audio(**eval(x)),
                description="ä¸Šä¼ å‚è€ƒéŸ³é¢‘ç”¨äºéŸ³è‰²å…‹éš†ã€‚å‚æ•°ï¼š{\"audio_bytes\": \"...\", \"description\": \"æè¿°\"}"
            ),
            Tool(
                name="synthesize_with_clone",
                func=lambda x: synthesize_with_clone(**eval(x)),
                description="ä½¿ç”¨å…‹éš†éŸ³è‰²åˆæˆè¯­éŸ³ã€‚å‚æ•°ï¼š{\"text\": \"ä½ å¥½\", \"emotion_tag\": \"(happy)\"}"
            ),
            Tool(
                name="optimize_voice",
                func=lambda x: optimize_voice(x),
                description="æ ¹æ®åé¦ˆä¼˜åŒ–éŸ³è‰²ã€‚å‚æ•°ï¼šåé¦ˆæè¿°å¦‚'å¤ªå¹´è½»äº†'ã€'éŸ³è°ƒå¤ªå°–'"
            ),
            Tool(
                name="get_optimization_tips",
                func=lambda x: get_optimization_tips(),
                description="è·å–å½“å‰ä¼˜åŒ–å»ºè®®å’Œå‚æ•°çŠ¶æ€"
            ),
            Tool(
                name="reset_voice_params",
                func=lambda x: reset_voice_params(),
                description="é‡ç½®æ‰€æœ‰å‚æ•°åˆ°é»˜è®¤å€¼"
            ),
        ]
    
    def _create_prompt(self):
        return ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯æ™ºèƒ½è¯­éŸ³åˆæˆåŠ©æ‰‹ï¼Œå¸®åŠ©ç”¨æˆ·å…‹éš†å’Œä¼˜åŒ–éŸ³è‰²ã€‚

å·¥ä½œæµç¨‹ï¼š
1. å¼•å¯¼ç”¨æˆ·ä¸Šä¼ å‚è€ƒéŸ³é¢‘ï¼ˆå»ºè®® 10-30 ç§’ï¼Œæ¸…æ™°æ— å™ªéŸ³ï¼‰
2. æé†’ç”¨æˆ·å¯ä»¥ä¸Šä¼ å¤šæ®µéŸ³é¢‘èåˆï¼Œæ•ˆæœæ›´å¥½
3. åˆæˆåè¯¢é—®ç”¨æˆ·åé¦ˆ
4. æ ¹æ®åé¦ˆä½¿ç”¨ optimize_voice è°ƒæ•´å‚æ•°
5. é‡æ–°åˆæˆï¼Œç›´åˆ°ç”¨æˆ·æ»¡æ„

å¯è°ƒå‚æ•°ï¼š
- pitch: éŸ³è°ƒé«˜ä½ï¼ˆ-10 ~ +10ï¼‰
- speed: è¯­é€Ÿå¿«æ…¢ï¼ˆ0.5 ~ 2.0ï¼‰
- timbre_depth: éŸ³è‰²åšåº¦ï¼ˆ-5 ~ +5ï¼‰
- age_shift: å¹´é¾„æ„Ÿï¼ˆ-5 æ›´å¹´è½» ~ +5 æ›´æˆç†Ÿï¼‰
- emotion_strength: æƒ…æ„Ÿå¼ºåº¦ï¼ˆ0.5 ~ 2.0ï¼‰

å¸¸è§åé¦ˆåŠå¤„ç†ï¼š
- "å¤ªå¹´è½»äº†" â†’ age_shift å¢åŠ 
- "å¤ªè€äº†" â†’ age_shift å‡å°‘
- "éŸ³è°ƒå¤ªå°–" â†’ pitch é™ä½ï¼Œtimbre_depth å¢åŠ 
- "å£°éŸ³å¤ªç²—" â†’ pitch æé«˜ï¼Œtimbre_depth å‡å°‘
- "è¯­é€Ÿå¤ªå¿«" â†’ speed é™ä½
- "è¯­é€Ÿå¤ªæ…¢" â†’ speed æé«˜
- "æ²¡æ„Ÿæƒ…" â†’ emotion_strength å¢åŠ 
"""),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ])
    
    def _save_audio(self, audio: bytes, prefix: str) -> str:
        import tempfile
        import os
        os.makedirs("outputs", exist_ok=True)
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav", dir="outputs", prefix=f"{prefix}_") as f:
            f.write(audio)
            return f.name
    
    async def run(self, user_input: str) -> Dict[str, Any]:
        result = await self.executor.ainvoke({"input": user_input})
        return result
